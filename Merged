import csv
import json
import subprocess
from datetime import datetime, timezone
from urllib.parse import quote

# Function to load API keys from a file
def load_api_keys(filename):
    api_keys = {}
    with open(filename, 'r') as file:
        for line in file:
            line = line.strip()
            if line:
                function_name, key = line.split(':', 1)
                api_keys[function_name] = key.strip()
    return api_keys

# Function to convert epoch time to a readable timestamp
def convert_epoch_to_timestamp(epoch_time):
    return datetime.fromtimestamp(int(epoch_time), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')

# Function to safely convert data to JSON
def safe_json_dumps(data):
    try:
        return json.dumps(data)
    except Exception as e:
        print("JSON Error:", e, data)
        return "{}"

# Function to execute curl command and return JSON data
def execute_curl_and_get_data(curl_command):
    result = subprocess.run(curl_command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Error executing curl command: {result.stderr}")
        return None
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response: {e}")
        return None

# Function to process metrics and write results directly to CSV
def process_metrics(input_files, api_keys, output_file):
    start_epoch = '1734310800'  # Example start time in epoch
    end_epoch = '1734318000'    # Example end time in epoch
    step = '1h'

    # List of labels to exclude
    excluded_keys = ['__name__', 'jobid', 'job', 'localdn',
                     'write_relabel_group', 'kubernetes_namespace',
                     'kubernetes_pod_name', 'applicationId']

    # Dictionary to temporarily store merged data
    merged_data = {}

    for input_file, function_name in input_files.items():
        encoded_api_key = api_keys.get(function_name)
        if not encoded_api_key:
            print(f"Skipping {function_name} due to missing API key.")
            continue

        with open(input_file, 'r') as file:
            for metric_name in file:
                metric_name = metric_name.strip()
                if not metric_name:
                    continue

                for query_type, column_name in [("sum_over_time", "metric_sum"), ("increase", "metric_increase")]:
                    query = f'{query_type}({metric_name}[1h])'
                    encoded_query = quote(query)
                    curl_command = (
                        f"curl --location --globoff \"https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1/query_range?"
                        f"query={encoded_query}&start={start_epoch}&end={end_epoch}&step={step}\""
                        f" --header \"Authorization: Basic {encoded_api_key}\""
                    )

                    print(f"Executing query for {metric_name} ({query_type}):\n{curl_command}")

                    response_data = execute_curl_and_get_data(curl_command)
                    if not response_data or response_data.get('status') != 'success':
                        print(f"No data found for {metric_name} ({query_type}).")
                        continue

                    for result in response_data['data']['result']:
                        fqdn = (
                            result['metric'].get('kubernetes_namespace', 'unknown')
                            if function_name in ['amf', 'smf']
                            else result['metric'].get('localdn', 'unknown')
                        )

                        # Filter out excluded keys
                        labels = {k: v for k, v in result['metric'].items() if k not in excluded_keys}

                        for value in result['values']:
                            event_time = convert_epoch_to_timestamp(value[0])
                            trans_dt = event_time.split(' ')[0]
                            metric_value = value[1]

                            # Create a unique key to identify rows
                            key = (metric_name, fqdn, event_time)

                            # Merge data into the dictionary
                            if key not in merged_data:
                                merged_data[key] = {
                                    "metric_name": metric_name,
                                    "FQDN": fqdn,
                                    "event_time": event_time,
                                    "trans_dt": trans_dt,
                                    "metric_sum": None,
                                    "metric_increase": None,
                                    "labels": safe_json_dumps(labels),
                                }

                            # Update the corresponding column
                            merged_data[key][column_name] = metric_value

    # Write merged results to CSV
    with open(output_file, mode='w', newline='') as csv_file:
        fieldnames = ["trans_dt", "metric_name", "FQDN", "event_time", "metric_sum", "metric_increase", "labels"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()

        for row in merged_data.values():
            writer.writerow(row)

    print(f"CSV file created successfully: {output_file}")

# Main execution block
if __name__ == "__main__":
    # Input files mapping to their respective functions
    input_files = {
        "smf_metrics.txt": "smf",
       # "upf_metrics.txt": "upf",
    }

    # API keys file
    api_keys_file = "api_keys.txt"

    # Output CSV file
    output_file = "metrics_results_inst.csv"

    # Load API keys and process metrics
    api_keys = load_api_keys(api_keys_file)
    process_metrics(input_files, api_keys, output_file)
