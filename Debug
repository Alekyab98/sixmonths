import csv
import json
import subprocess
from datetime import datetime, timezone
from urllib.parse import quote
import requests

# Configuration
BASE_URL = "https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1"
AUTHORIZATION = "Basic <your_encoded_key>"  # Replace with your encoded API key
HEADERS = {
    "Authorization": AUTHORIZATION,
    "Accept": "application/json"
}

# Function to load API keys from a file
def load_api_keys(filename):
    api_keys = {}
    with open(filename, 'r') as file:
        for line in file:
            line = line.strip()
            if line:
                function_name, key = line.split(':', 1)
                api_keys[function_name] = key.strip()
    return api_keys

# Function to convert epoch time to readable UTC timestamp
def convert_epoch_to_timestamp(epoch_time):
    return datetime.fromtimestamp(int(epoch_time), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')

# Function to fetch all available metrics (for debugging)
def fetch_all_metrics():
    url = f"{BASE_URL}/label/__name__/values"
    response = requests.get(url, headers=HEADERS)

    if response.status_code != 200:
        print(f"Error: Unable to fetch metrics. HTTP {response.status_code}")
        return []

    data = response.json()
    return data.get("data", [])

# Debugging function for empty results
def debug_query(metric_name, start, end, step):
    query = f"sum_over_time({metric_name}[1h])"
    url = f"{BASE_URL}/query_range?query={query}&start={start}&end={end}&step={step}"
    print(f"Querying: {url}")

    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        print(f"Error: HTTP {response.status_code}")
        print(response.text)
        return

    data = response.json()
    if data.get("status") != "success":
        print("Error in API response:", data)
        return

    results = data.get("data", {}).get("result", [])
    if not results:
        print("No data found for metric:", metric_name)
    else:
        print(f"Data found for metric: {metric_name}")
        for item in results:
            labels = item.get("metric", {})
            for value in item.get("values", []):
                timestamp, metric_value = value
                print(f"  Time: {convert_epoch_to_timestamp(timestamp)}, Value: {metric_value}")

# Function to execute curl command and get raw JSON data
def execute_curl_and_get_data(curl_command):
    result = subprocess.run(curl_command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Error executing curl command: {result.stderr}")
        return None
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response: {e}")
        return None

# Main function to combine and debug metrics
def fetch_combined_metrics(input_files, api_keys, output_file):
    start_epoch = '1719792000'  # Example start time
    end_epoch = '1719828000'    # Example end time
    step = '1h'

    # Fetch all available metrics for debugging
    all_metrics = fetch_all_metrics()
    print(f"Total metrics available: {len(all_metrics)}")

    combined_data_rows = {}

    with open(output_file, mode='w', newline='') as csv_file:
        fieldnames = ["metric_name", "FQDN", "labels", "event_time", "transdt", "metric_sum", "metric_increase"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()

        for input_file, function_name in input_files.items():
            encoded_api_key = api_keys.get(function_name)
            if not encoded_api_key:
                print(f"Skipping {function_name} due to missing API key.")
                continue

            with open(input_file, 'r') as file:
                for metric_name in file:
                    metric_name = metric_name.strip()
                    if not metric_name:
                        continue

                    # Check if the metric exists
                    if metric_name not in all_metrics:
                        print(f"Metric '{metric_name}' not found in available metrics. Skipping...")
                        continue

                    # Debug the metric query
                    print(f"Debugging metric: {metric_name}")
                    debug_query(metric_name, start_epoch, end_epoch, step)

                    # Perform additional processing if required...

    print(f"Combined data written to CSV: {output_file}")

# Main Execution Block
if __name__ == "__main__":
    input_files = {
        "upf_metrics.txt": "upf",
    }

    # API keys file
    api_keys_file = "api_keys.txt"

    # Output CSV file
    output_file = "upf_data.csv"

    # Load API keys and process metrics
    api_keys = load_api_keys(api_keys_file)
    fetch_combined_metrics(input_files, api_keys, output_file)
