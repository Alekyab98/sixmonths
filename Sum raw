import csv
import json
import subprocess
from datetime import datetime, timezone
from urllib.parse import quote

# Function to load API keys from a file
def load_api_keys(filename):
    api_keys = {}
    with open(filename, 'r') as file:
        for line in file:
            line = line.strip()
            if line:
                function_name, key = line.split(':', 1)
                api_keys[function_name] = key.strip()
    return api_keys

# Function to convert epoch time to readable UTC timestamp
def convert_epoch_to_timestamp(epoch_time):
    return datetime.fromtimestamp(int(epoch_time), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')

# Function to extract date from timestamp (YYYY-MM-DD format)
def extract_date_from_timestamp(timestamp):
    return timestamp.split(' ')[0]

# Function to safely convert data to JSON
def safe_json_dumps(data):
    try:
        return json.dumps(data)
    except Exception as e:
        print("JSON Error:", e, data)
        return "{}"

# Function to execute curl command and get raw JSON data
def execute_curl_and_get_data(curl_command):
    result = subprocess.run(curl_command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Error executing curl command: {result.stderr}")
        return None
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response: {e}")
        return None

# Function to process metrics and combine sum_over_time and increase results
def fetch_combined_metrics(input_files, api_keys, output_file):
    start_epoch = '1734310800'  # Example start time
    end_epoch = '1734318000'    # Example end time
    step = '1h'

    data_rows = {}

    # Open CSV file to write combined results
    with open(output_file, mode='w', newline='') as csv_file:
        fieldnames = ["metric_name", "FQDN", "event_time", "transdt", "metric_sum", "metric_increase", "labels"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()

        for input_file, function_name in input_files.items():
            encoded_api_key = api_keys.get(function_name)
            if not encoded_api_key:
                print(f"Skipping {function_name} due to missing API key.")
                continue

            with open(input_file, 'r') as file:
                for metric_name in file:
                    metric_name = metric_name.strip()
                    if not metric_name:
                        continue

                    for query_type, column_name in [("sum_over_time", "metric_sum"), ("increase", "metric_increase")]:
                        query = f'{query_type}({metric_name}[1h])'
                        encoded_query = quote(query)
                        curl_command = (
                            f"curl --location --globoff \"https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1/query_range?"
                            f"query={encoded_query}&start={start_epoch}&end={end_epoch}&step={step}\""
                            f" --header \"Authorization: Basic {encoded_api_key}\""
                        )

                        print(f"Executing query for {metric_name} ({query_type}):\n{curl_command}")
                        response_data = execute_curl_and_get_data(curl_command)

                        if not response_data or response_data.get('status') != 'success':
                            print(f"No data found for {metric_name} ({query_type}).")
                            continue

                        for result in response_data['data']['result']:
                            fqdn = result['metric'].get('kubernetes_namespace', result['metric'].get('localdn', 'unknown'))

                            # Remove fields already captured in other columns
                            labels = {k: v for k, v in result['metric'].items() if k not in ['__name__', 'jobid', 'job', 'localdn', 'instance', 'write_relabel_group', 'kubernetes_namespace', 'kubernetes_pod_name', 'applicationId', 'metric_name']}

                            for value in result['values']:
                                event_time = convert_epoch_to_timestamp(value[0])
                                transdt = extract_date_from_timestamp(event_time)  # Extract date from event_time
                                metric_value = value[1]

                                # Create a unique key for combining rows
                                key = (metric_name, fqdn, event_time)

                                if key not in data_rows:
                                    data_rows[key] = {
                                        "metric_name": metric_name,
                                        "FQDN": fqdn,
                                        "event_time": event_time,
                                        "transdt": transdt,
                                        "metric_sum": None,
                                        "metric_increase": None,
                                        "labels": safe_json_dumps(labels)
                                    }

                                # Add the appropriate value to the correct column
                                data_rows[key][column_name] = metric_value

        # Write combined rows to the CSV
        for row in data_rows.values():
            writer.writerow(row)

    print(f"Combined data written to CSV: {output_file}")

# Main execution block
if __name__ == "__main__":
    # Input files mapping to their respective functions
    input_files = {
        "smf_metrics.txt": "smf",
        "upf_metrics.txt": "upf",
    }

    # API keys file
    api_keys_file = "api_keys.txt"

    # Output CSV file
    output_file = "combined_metrics_results.csv"

    # Load API keys and process metrics
    api_keys = load_api_keys(api_keys_file)
    fetch_combined_metrics(input_files, api_keys, output_file)
