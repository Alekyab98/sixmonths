def fetch_metric_data_separately(curl_command, metric_name, column_name):
    """Fetch data for a single metric and return normalized rows."""
    response_data = execute_curl_and_get_data(curl_command)
    if not response_data or response_data.get('status') != 'success':
        print(f"No data found for query: {curl_command}")
        return []

    data_rows = []

    for result in response_data['data']['result']:
        labels = {k: v for k, v in result['metric'].items()}  # Extract all labels
        fqdn = result['metric'].get('kubernetes_namespace', result['metric'].get('localdn', 'unknown'))  # Extract FQDN
        for value in result['values']:
            event_time = convert_epoch_to_timestamp(value[0])
            transdt = extract_date_from_timestamp(event_time)
            metric_value = value[1]

            # Create a normalized row
            row = {
                "metric_name": metric_name,
                "FQDN": fqdn,
                "labels": safe_json_dumps(labels),
                "event_time": event_time,
                "transdt": transdt,
                "metric_sum": None,
                "metric_increase": None
            }
            # Assign the column value
            row[column_name] = metric_value
            data_rows.append(row)

    return data_rows

def fetch_combined_metrics_individually(input_files, api_keys, output_file_base, max_rows_per_file=100000):
    """Fetch and combine metrics by processing them individually."""
    start_epoch = '1719792000'  # Example start time
    end_epoch = '1725490800'    # Example end time
    step = '1h'

    file_count = 1
    row_count = 0
    output_file = f"{output_file_base}_{file_count}.csv"
    csv_file = open(output_file, mode='w', newline='')
    fieldnames = ["metric_name", "FQDN", "labels", "event_time", "transdt", "metric_sum", "metric_increase"]
    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
    writer.writeheader()

    for input_file, function_name in input_files.items():
        encoded_api_key = api_keys.get(function_name)
        if not encoded_api_key:
            print(f"Skipping {function_name} due to missing API key.")
            continue

        with open(input_file, 'r') as file:
            for metric_name in file:
                metric_name = metric_name.strip()
                if not metric_name:
                    continue

                # Query A: sum_over_time
                query_sum = f'sum_over_time({metric_name}[1h])'
                curl_command_sum = (
                    f"curl --location --globoff \"https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1/query_range?"
                    f"query={quote(query_sum)}&start={start_epoch}&end={end_epoch}&step={step}\""
                    f" --header \"Authorization: Basic {encoded_api_key}\""
                )
                sum_data = fetch_metric_data_separately(curl_command_sum, metric_name, "metric_sum")

                # Query B: increase
                query_increase = f'increase({metric_name}[1h])'
                curl_command_increase = (
                    f"curl --location --globoff \"https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1/query_range?"
                    f"query={quote(query_increase)}&start={start_epoch}&end={end_epoch}&step={step}\""
                    f" --header \"Authorization: Basic {encoded_api_key}\""
                )
                increase_data = fetch_metric_data_separately(curl_command_increase, metric_name, "metric_increase")

                # Merge the two sets of data
                combined_data = {}
                for row in sum_data + increase_data:
                    key = (row["metric_name"], row["FQDN"], row["event_time"])
                    if key not in combined_data:
                        combined_data[key] = row
                    else:
                        combined_data[key].update(row)

                # Write to the current CSV file
                for row in combined_data.values():
                    writer.writerow(row)
                    row_count += 1

                    # Check if we need to create a new file
                    if row_count >= max_rows_per_file:
                        csv_file.close()
                        file_count += 1
                        row_count = 0
                        output_file = f"{output_file_base}_{file_count}.csv"
                        csv_file = open(output_file, mode='w', newline='')
                        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
                        writer.writeheader()

    csv_file.close()
    print(f"Combined data written to CSV files with a maximum of {max_rows_per_file} rows per file.")
